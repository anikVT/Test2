import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, seq_len=5, num_tokens=230, embed_dim=768, use_spatial=True):
        super(PositionalEncoding, self).__init__()
        self.embed_dim = embed_dim
        self.use_spatial = use_spatial
        
        # Temporal positional encoding (shape: [1, seq_len, 1, embed_dim])
        pe_time = torch.zeros(seq_len, embed_dim)
        position_t = torch.arange(0, seq_len).unsqueeze(1)  # (seq_len, 1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))
        pe_time[:, 0::2] = torch.sin(position_t * div_term)
        pe_time[:, 1::2] = torch.cos(position_t * div_term)
        pe_time = pe_time.unsqueeze(0).unsqueeze(2)  # (1, seq_len, 1, embed_dim)
        self.register_buffer('pe_time', pe_time)

        if use_spatial:
            # Spatial positional encoding (shape: [1, 1, num_tokens, embed_dim])
            pe_space = torch.zeros(num_tokens, embed_dim)
            position_s = torch.arange(0, num_tokens).unsqueeze(1)
            pe_space[:, 0::2] = torch.sin(position_s * div_term)
            pe_space[:, 1::2] = torch.cos(position_s * div_term)
            pe_space = pe_space.unsqueeze(0).unsqueeze(1)  # (1, 1, num_tokens, embed_dim)
            self.register_buffer('pe_space', pe_space)
        else:
            self.pe_space = None

    def forward(self, x):
        # x shape: (batch_size, seq_len, num_tokens, embed_dim)
        x = x + self.pe_time  # Add temporal encoding
        if self.use_spatial:
            x = x + self.pe_space  # Add spatial encoding
        return x



# For attention: (B*N, T, D)
input = input.permute(0, 2, 1, 3).reshape(B*N, T, D)  # shape: (2*230, 5, 768)

mask = torch.triu(torch.ones(T, T) * float('-inf'), diagonal=1).to(input.device)

# Apply attention with mask to preserve autoregressive behavior
attn_output = self_attn(query=input, key=input, value=input, attn_mask=mask)


---+
# Input: (B, T, N, D)
x = x.view(B, T*N, D)  # Shape: (B, T*N, D)



import torch

B, T, N, D = 2, 5, 230, 768
seq_len = T * N  # Total tokens per sequence

# Step 1: Get time indices for each token
# Each group of N tokens belongs to 1 time step
time_indices = torch.arange(T).repeat_interleave(N)  # Shape: (T*N,)

# Step 2: Compare time indices to build causal mask
# We want: token i can only attend to token j if time[j] <= time[i]
causal_mask = (time_indices[None, :] <= time_indices[:, None])  # Shape: (T*N, T*N)

# Step 3: Convert to float mask for attention
attn_mask = torch.where(causal_mask, torch.zeros_like(causal_mask, dtype=torch.float),
                        torch.full_like(causal_mask, float('-inf')))



from torch.nn import MultiheadAttention

mha = MultiheadAttention(embed_dim=D, num_heads=8, batch_first=True)

# Input x: (B, T*N, D), mask: (T*N, T*N)
attn_output, attn_weights = mha(x, x, x, attn_mask=attn_mask)
